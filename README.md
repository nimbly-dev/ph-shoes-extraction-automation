# PH Shoe Extractor Automation

Scrape well-known Shoes brand on PH Brand Sites
---

## ðŸ§± Project Structure

```bash
shoe-extractor-automation/
â”‚
â”œâ”€â”€ airflow_dags/             # Airflow DAGs and orchestration logic
â”‚   â”œâ”€â”€ dags/                 # Python DAG files
â”‚   â”œâ”€â”€ Dockerfile            # Airflow dev container
â”‚   â””â”€â”€ requirements.txt      # Airflow dependencies
â”‚
â”œâ”€â”€ lambda_extract/           # AWS Lambda scraping logic
â”‚   â”œâ”€â”€ extractors/           # Brand-specific scrapers (Asics, Nike, etc.)
â”‚   â”œâ”€â”€ lambda_handler.py     # Lambda entry point
â”‚   â”œâ”€â”€ Dockerfile            # For local Lambda dev/testing
â”‚   â”œâ”€â”€ test_runner.py        # Run extractors locally
â”‚   â””â”€â”€ requirements.txt      # Lambda runtime dependencies
â”‚
â”œâ”€â”€ glue_jobs/                # AWS Glue ETL jobs for data cleaning/transform
â”‚   â”œâ”€â”€ clean_*.py            # PySpark Glue jobs
â”‚   â”œâ”€â”€ Dockerfile            # Glue local test environment
â”‚   â””â”€â”€ glue_requirements.txt # Additional packages for Glue (optional)
â”‚
â”œâ”€â”€ scripts/                  # Utility scripts
â”‚   â””â”€â”€ package_lambda.sh     # Builds Lambda .zip package for deployment
â”‚
â”œâ”€â”€ terraform/                # Infrastructure as code definitions (Lambda, Glue, S3, etc.)
â”‚   â”œâ”€â”€ main.tf
â”‚   â”œâ”€â”€ lambda.tf
â”‚   â”œâ”€â”€ glue.tf
â”‚   â”œâ”€â”€ s3.tf
â”‚   â”œâ”€â”€ variables.tf
â”‚   â””â”€â”€ outputs.tf
â”‚
â”œâ”€â”€ .github/                  # CI/CD workflows
â”‚   â””â”€â”€ workflows/
â”‚       â”œâ”€â”€ deploy_lambda.yml
â”‚       â”œâ”€â”€ deploy_airflow.yml
â”‚       â””â”€â”€ deploy_glue.yml
â”‚
â”œâ”€â”€ docker-compose.yml        # Compose file to spin up Airflow, Lambda dev, etc.
â””â”€â”€ README.md


Services Users:

1. ph-shoes-terraform-user: Terraform user



Supported on Lambda Daily Extraction Automation:

1. Hoka
2. Nike
3. World Balance

Not Supported but present on Local Web Extractors

1. Asics
2. New Balance 
3. Adidas


sudo tail -F /var/log/aws/codedeploy-agent/codedeploy-agent.log


Clean codedeploy:

# 1) Stop the agent so it doesnâ€™t fight you over files
sudo systemctl stop codedeploy-agent

# 2) Remove all archived revisions (old bundles + unpacked dirs)
sudo rm -rf /opt/codedeploy-agent/deployment-root/*

# 3) (Optional) Clear rotated logs if you need more room
sudo rm -rf /var/log/aws/codedeploy-agent/*

# 4) Restart agent
sudo systemctl start codedeploy-agent

.\deploy_airflow_local.ps1 -SkipUpload

docker rm -f airflow-scheduler 2>/dev/null || true

docker logs -f airflow-scheduler

docker exec -it airflow-scheduler airflow dags trigger ph_shoes_etl

docker exec -it airflow-scheduler airflow dags list

docker exec -it airflow-scheduler \
  airflow dags unpause ph_shoes_etl


Airflow Deployment Step

1. Build airflow_dags project using Docker, put it on .tar ball
2. Build deployment.zip, include .tar ball, and scripts, appspec.yml
3. Put to S3 Airflow Artifacts 
4. Provision EC2 Instances, wait for it to boot-up (Ensure that Old Instances is removed after new EC2 Startup)
5. Run Codedeploy


terraform-core: Contains provisioning modules for project e.g iAM for EC2, S3 Bucket for Artifacts, VPC Group connection
terraform-ec2-airflow: Contains EC2 Template where airflow code will be deployed.

terraform output redshift_admin_password

$ aws secretsmanager describe-secret \
   --secret-id prod/ph-shoes/redshift-master \
   --region ap-southeast-1 \
   --query 'ARN' \
   --output text