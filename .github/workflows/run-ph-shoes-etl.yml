# .github/workflows/run-ph-shoes-etl.yml
name: Run ph_shoes_etl & dbt

on:
  workflow_dispatch:
    inputs:
      dag_id:
        description: 'Airflow DAG ID'
        required: true
        default: 'ph_shoes_etl'
      run_dbt:
        description: 'Trigger dbt Cloud downstream?'
        required: true
        type: choice
        options: ['true','false']
        default: 'true'

env:
  AWS_REGION:        ${{ vars.TF_VAR_AWS_REGION }}
  EC2_INSTANCE_NAME: ${{ vars.TF_VAR_EC2_INSTANCE_NAME }}

  # Airflow creds from Secrets Manager → you could also store these as GitHub secrets
  AIRFLOW_USER:      ${{ secrets.AIRFLOW_USER }}
  AIRFLOW_PASSWORD:  ${{ secrets.AIRFLOW_PASSWORD }}

  # dbt Cloud
  DBT_ACCOUNT_ID:    ${{ secrets.DBT_CLOUD_ACCOUNT_ID }}
  DBT_JOB_ID:        ${{ secrets.DBT_CLOUD_JOB_ID }}
  DBT_API_TOKEN:     ${{ secrets.DBT_API_TOKEN }}

jobs:
  run-and-poll:
    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v3

    - name: Configure AWS creds
      uses: aws-actions/configure-aws-credentials@v3
      with:
        aws-access-key-id:     ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region:            ${{ env.AWS_REGION }}

    - name: Resolve EC2 Public IP
      id: host
      run: |
        IP=$(aws ec2 describe-instances \
          --filters Name=tag:Name,Values="${EC2_INSTANCE_NAME}" Name=instance-state-name,Values=running \
          --query 'Reservations[0].Instances[0].PublicIpAddress' --output text)
        echo "host=http://$IP:8080" >> $GITHUB_OUTPUT

    - name: Unpause DAG
      run: |
        curl -fsS -X PATCH \
          "${{ steps.host.outputs.host }}/api/v1/dags/${{ github.event.inputs.dag_id }}" \
          -u "${AIRFLOW_USER}:${AIRFLOW_PASSWORD}" \
          -H "Content-Type: application/json" \
          -d '{"is_paused":false}'

    - name: Trigger DAG run
      id: trigger
      run: |
        RUN_ID="gh-$(date +%s)"
        RESPONSE=$(curl -fsS -X POST \
          "${{ steps.host.outputs.host }}/api/v1/dags/${{ github.event.inputs.dag_id }}/dagRuns" \
          -u "${AIRFLOW_USER}:${AIRFLOW_PASSWORD}" \
          -H "Content-Type: application/json" \
          -d '{"dag_run_id":"'"$RUN_ID"'","conf":{}}')
        echo "$RESPONSE" | jq
        echo "run_id=$RUN_ID" >> $GITHUB_OUTPUT

    - name: Wait for DAG to finish
      run: |
        for i in $(seq 1 120); do
          STATE=$(curl -fsS \
            "${{ steps.host.outputs.host }}/api/v1/dags/${{ github.event.inputs.dag_id }}/dagRuns/${{ steps.trigger.outputs.run_id }}" \
            -u "${AIRFLOW_USER}:${AIRFLOW_PASSWORD}" \
            | jq -r .state)
          echo "Attempt #$i → $STATE"
          if [[ "$STATE" == success ]]; then exit 0; fi
          if [[ "$STATE" == failed  ]]; then exit 1; fi
          sleep 15
        done
        echo "❌ Timed out waiting for DAG" >&2
        exit 1

    - name: Trigger dbt Cloud job
      if: ${{ github.event.inputs.run_dbt == 'true' }}
      run: |
        curl -fsS -X POST \
          "https://cloud.getdbt.com/api/v2/accounts/${DBT_ACCOUNT_ID}/jobs/${DBT_JOB_ID}/run/" \
          -H "Authorization: Token ${DBT_API_TOKEN}" \
          -H "Content-Type: application/json" \
          -d '{"cause":"Triggered after DAG"}' \
        | jq

    - name: Wait for dbt run
      if: ${{ github.event.inputs.run_dbt == 'true' }}
      run: |
        for i in {1..60}; do
          STATUS=$(curl -fsS \
            "https://cloud.getdbt.com/api/v2/accounts/${DBT_ACCOUNT_ID}/runs/$(jq -nr "${{ steps.dbt.outputs.dbt_run_id }}")" \
            -H "Authorization: Token ${DBT_API_TOKEN}" \
            | jq -r .data.attributes.status)
          echo "dbt attempt $i → $STATUS"
          [[ "$STATUS" == Success ]] && exit 0
          [[ "$STATUS" == Error   ]] && exit 1
          sleep 10
        done
        echo "❌ dbt run timed out" >&2
        exit 1
