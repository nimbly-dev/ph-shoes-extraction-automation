name: Run ph_shoes_etl DAG (and then dbt)

on:
  workflow_dispatch:
    inputs:
      dag_id:
        description: 'Airflow DAG ID'
        required: true
        default: 'ph_shoes_etl'
      dbt_year:
        description: 'Year for dbt models (YYYY)'
        required: false
        default: ''
      dbt_month:
        description: 'Month for dbt models (MM)'
        required: false
        default: ''
      dbt_day:
        description: 'Day for dbt models (DD)'
        required: false
        default: ''

env:
  AWS_REGION:        ap-southeast-1
  EC2_INSTANCE_NAME: airflow-ec2

jobs:
  trigger-and-dbt:
    runs-on: ubuntu-latest
    environment: main

    steps:
      - name: Checkout repo
        uses: actions/checkout@v3

      - name: Configure AWS credentials for SSM
        uses: aws-actions/configure-aws-credentials@v3
        with:
          aws-access-key-id:     ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region:            ${{ env.AWS_REGION }}

      - name: Resolve EC2 instance ID
        id: get_id
        run: |
          ID=$(aws ec2 describe-instances \
            --filters "Name=tag:Name,Values=${EC2_INSTANCE_NAME}" \
                      "Name=instance-state-name,Values=running" \
            --query "Reservations[0].Instances[0].InstanceId" \
            --output text)
          echo "id=$ID" >> $GITHUB_OUTPUT

      - name: (Debug) List loaded DAGs
        run: |
          INSTANCE_ID=${{ steps.get_id.outputs.id }}
          CMD=$(aws ssm send-command \
            --instance-ids "$INSTANCE_ID" \
            --document-name AWS-RunShellScript \
            --comment "Debug: list dags" \
            --parameters commands='["docker exec airflow-scheduler airflow dags list"]' \
            --region $AWS_REGION \
            --query "Command.CommandId" --output text)
          aws ssm wait command-executed --command-id "$CMD" --instance-id "$INSTANCE_ID" --region $AWS_REGION
          aws ssm get-command-invocation --command-id "$CMD" --instance-id "$INSTANCE_ID" --region $AWS_REGION --output json

      - name: Send SSM command (unpause & trigger)
        id: send
        run: |
          INSTANCE_ID=${{ steps.get_id.outputs.id }}
          DAG_ID=${{ github.event.inputs.dag_id }}

          echo "▶ Unpause & trigger $DAG_ID on $INSTANCE_ID"
          CMD_ID=$(aws ssm send-command \
            --instance-ids "$INSTANCE_ID" \
            --document-name AWS-RunShellScript \
            --comment "Trigger $DAG_ID DAG" \
            --parameters commands='[
              "docker exec airflow-scheduler airflow dags unpause '"$DAG_ID"'",
              "docker exec airflow-scheduler airflow dags trigger '"$DAG_ID"' --run-id gh-action-'"$(date +%s)"'"
            ]' \
            --region $AWS_REGION \
            --query "Command.CommandId" --output text)

          echo "cmd_id=$CMD_ID" >> $GITHUB_OUTPUT

      - name: Wait for Airflow SSM trigger to finish
        run: |
          INSTANCE_ID=${{ steps.get_id.outputs.id }}
          CMD_ID=${{ steps.send.outputs.cmd_id }}

          for i in $(seq 1 60); do
            STATUS=$(aws ssm get-command-invocation \
              --command-id "$CMD_ID" \
              --instance-id "$INSTANCE_ID" \
              --region $AWS_REGION \
              --query "Status" --output text)
            if [ "$STATUS" = "InProgress" ]; then
              sleep 5
              continue
            fi
            echo "▶ SSM trigger step finished with status=$STATUS"
            if [ "$STATUS" != "Success" ]; then
              aws ssm get-command-invocation --command-id "$CMD_ID" --instance-id "$INSTANCE_ID" --region $AWS_REGION --output json
              exit 1
            fi
            break
          done

      - name: Wait for DAG run to complete
        run: |
          INSTANCE_ID=${{ steps.get_id.outputs.id }}
          DAG_ID=${{ github.event.inputs.dag_id }}
          RUN_ID="gh-action-${{ steps.send.outputs.cmd_id }}"

          echo "⏳ Waiting for $DAG_ID run $RUN_ID to complete…"
          for i in $(seq 1 60); do
            # always return JSON (empty list if not yet present)
            LIST_JSON=$(aws ssm send-command \
              --instance-ids "$INSTANCE_ID" \
              --document-name AWS-RunShellScript \
              --comment "Check DAG run $RUN_ID" \
              --parameters commands='["docker exec airflow-scheduler bash -lc \"airflow dags list-runs -d '"$DAG_ID"' --output json || echo []\""]' \
              --region $AWS_REGION \
              --query "Command.CommandId" --output text \
              | xargs -I{} aws ssm wait command-executed --command-id {} --instance-id "$INSTANCE_ID" --region $AWS_REGION \
                && aws ssm get-command-invocation --command-id {} --instance-id "$INSTANCE_ID" --region $AWS_REGION --query "StandardOutputContent" --output text)

            STATE=$(echo "$LIST_JSON" | jq -r '.[] | select(.run_id=="'"$RUN_ID"'") | .state // ""')
            echo "Attempt #$i: run state = '${STATE:-queued}'"
            if [ "$STATE" = "success" ]; then
              echo "✅ DAG run succeeded"
              exit 0
            elif [ "$STATE" = "failed" ]; then
              echo "❌ DAG run failed"
              exit 1
            fi
            sleep 10
          done

          echo "⏰ Timed out waiting for DAG run"
          exit 1

      - name: Fetch SSM invocation result
        if: always()
        run: |
          aws ssm get-command-invocation \
            --command-id "${{ steps.send.outputs.cmd_id }}" \
            --instance-id "${{ steps.get_id.outputs.id }}" \
            --region $AWS_REGION \
            --output json

      - name: Set DBT date variables
        run: |
          YIN=${{ github.event.inputs.dbt_year }}
          MIN=${{ github.event.inputs.dbt_month }}
          DIN=${{ github.event.inputs.dbt_day }}

          Y=${YIN:-$(date +'%Y')}
          M=${MIN:-$(date +'%m')}
          D=${DIN:-$(date +'%d')}

          echo "DBT_YEAR=$Y"  >> $GITHUB_ENV
          echo "DBT_MONTH=$M" >> $GITHUB_ENV
          echo "DBT_DAY=$D"   >> $GITHUB_ENV

      - name: Install dbt and run model
        env:
          DBT_YEAR:  ${{ env.DBT_YEAR }}
          DBT_MONTH: ${{ env.DBT_MONTH }}
          DBT_DAY:   ${{ env.DBT_DAY }}
        run: |
          python -m pip install --upgrade pip
          pip install --no-cache-dir dbt-core dbt-postgres

          cd path/to/your/dbt/project

          dbt deps
          dbt source freshness

          dbt run \
            --models +fact_product_shoes \
            --vars "{\"year\":${DBT_YEAR},\"month\":${DBT_MONTH},\"day\":${DBT_DAY}}"
