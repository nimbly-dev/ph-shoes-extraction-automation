name: Run ph_shoes_etl DAG & trigger dbt Cloud

on:
  workflow_dispatch:
    inputs:
      dag_id:
        description: 'Airflow DAG ID'
        required: true
        default: 'ph_shoes_etl'
      run_dbt:
        description: 'Also trigger the downstream dbt Cloud job?'
        required: true
        default: 'true'
        type: choice
        # GitHub only supports strings for choice, so use 'true'/'false'
        options:
          - 'true'
          - 'false'

env:
  AWS_REGION:        ap-southeast-1
  EC2_INSTANCE_NAME: airflow-ec2

jobs:
  run-and-trigger:
    runs-on: ubuntu-latest
    environment: main

    steps:
    - name: Checkout
      uses: actions/checkout@v3

    - name: Configure AWS creds for SSM
      uses: aws-actions/configure-aws-credentials@v3
      with:
        aws-access-key-id:     ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region:            ${{ env.AWS_REGION }}

    - name: Resolve EC2 instance ID
      id: get_id
      run: |
        ID=$(aws ec2 describe-instances \
          --filters "Name=tag:Name,Values=${EC2_INSTANCE_NAME}" \
                    "Name=instance-state-name,Values=running" \
          --query "Reservations[0].Instances[0].InstanceId" \
          --output text)
        echo "id=$ID" >> $GITHUB_OUTPUT

    - name: Trigger Airflow DAG via SSM
      id: send
      run: |
        INSTANCE_ID=${{ steps.get_id.outputs.id }}
        DAG_ID=${{ github.event.inputs.dag_id }}
        RUN_ID="gh-action-$(date +%s)"

        echo "▶ Unpause & trigger DAG $DAG_ID on $INSTANCE_ID (run-id=$RUN_ID)"
        CMD_ID=$(aws ssm send-command \
          --instance-ids "$INSTANCE_ID" \
          --document-name AWS-RunShellScript \
          --comment "Trigger $DAG_ID DAG" \
          --parameters commands='["docker exec airflow-scheduler airflow dags unpause '"$DAG_ID"'", "docker exec airflow-scheduler airflow dags trigger '"$DAG_ID"' --run-id '"$RUN_ID"'"]' \
          --region $AWS_REGION \
          --query "Command.CommandId" --output text)

        echo "cmd_id=$CMD_ID" >> $GITHUB_OUTPUT

    - name: Wait for Airflow trigger to finish
      run: |
        INSTANCE_ID=${{ steps.get_id.outputs.id }}
        CMD_ID=${{ steps.send.outputs.cmd_id }}

        for i in $(seq 1 60); do
          STATUS=$(aws ssm get-command-invocation \
            --command-id "$CMD_ID" \
            --instance-id "$INSTANCE_ID" \
            --region $AWS_REGION \
            --query "Status" --output text)
          if [[ "$STATUS" == "InProgress" ]] || [[ "$STATUS" == "Pending" ]]; then
            sleep 5
            continue
          fi
          echo "▶ SSM trigger finished with status=$STATUS"
          if [[ "$STATUS" != "Success" ]]; then
            aws ssm get-command-invocation \
              --command-id "$CMD_ID" \
              --instance-id "$INSTANCE_ID" \
              --region $AWS_REGION \
              --output json
            exit 1
          fi
          break
        done

    - name: Wait for DAG run to complete
      run: |
        INSTANCE_ID=${{ steps.get_id.outputs.id }}
        DAG_ID=${{ github.event.inputs.dag_id }}
        RUN_ID="gh-action-${{ steps.send.outputs.cmd_id }}"

        echo "▶ Giving the scheduler 15 s to register the run…"
        sleep 15

        echo "⏳ Polling up to 30× for $DAG_ID run-id $RUN_ID → success/failed…"
        for attempt in $(seq 1 30); do
          # 1) Send a fresh list-runs via SSM.
          #    We pass a single shell string in JSON so bash -lc runs correctly.
          CMD_ID=$(aws ssm send-command \
            --instance-ids "$INSTANCE_ID" \
            --document-name "AWS-RunShellScript" \
            --comment "Check DAG run $RUN_ID" \
            --parameters commands=["#!/bin/bash -lc 'airflow dags list-runs -d $DAG_ID --no-color --output json --limit 50 || echo []'"] \
            --region $AWS_REGION \
            --query "Command.CommandId" \
            --output text)

          # 2) Wait for that invocation to finish
          for p in $(seq 1 12); do
            SSM_STAT=$(aws ssm get-command-invocation \
              --command-id "$CMD_ID" \
              --instance-id "$INSTANCE_ID" \
              --region $AWS_REGION \
              --query "Status" \
              --output text)
            [[ "$SSM_STAT" =~ ^(Pending|InProgress)$ ]] && sleep 5 && continue
            break
          done

          # 3) Grab the raw JSON (or empty array)
          RAW=$(aws ssm get-command-invocation \
            --command-id "$CMD_ID" \
            --instance-id "$INSTANCE_ID" \
            --region $AWS_REGION \
            --query "StandardOutputContent" \
            --output text || echo "[]")

          echo "  ▶ RAW (attempt #$attempt):"
          echo "$RAW" | sed 's/^/    /'

          # 4) Strip ANSI, pull the JSON array
          JSON=$(echo "$RAW" \
            | sed -E 's/\x1B\[[0-9;]*[JKmsu]//g' \
            | awk '/^\[/{ print; exit }' \
            || echo "[]")

          # 5) Extract our run’s state
          STATE=$(echo "$JSON" | jq -r \
            '.[] | select(.run_id=="'"$RUN_ID"'") | .state // ""')

          echo "  → parsed state='${STATE:-queued}'"
          if [[ "$STATE" == "success" ]]; then
            echo "✅ DAG run succeeded"
            exit 0
          elif [[ "$STATE" == "failed" ]]; then
            echo "❌ DAG run failed"
            exit 1
          fi

          sleep 15
        done

        echo "⏰ Timed out waiting for DAG run"
        exit 1



    - name: Trigger dbt Cloud job
      if: ${{ github.event.inputs.run_dbt == 'true' }}
      env:
        ACCOUNT_ID: ${{ secrets.DBT_CLOUD_ACCOUNT_ID }}
        JOB_ID:     ${{ secrets.DBT_CLOUD_JOB_ID }}
        API_TOKEN:  ${{ secrets.DBT_API_TOKEN }}
      run: |
        echo "▶ Triggering dbt Cloud job $JOB_ID on account $ACCOUNT_ID…"
        curl -sSf -X POST "https://wz482.us1.dbt.com/api/v2/accounts/${ACCOUNT_ID}/jobs/${JOB_ID}/run/" \
          -H "Authorization: Token ${API_TOKEN}" \
          -H "Content-Type: application/json" \
          -d '{"cause":"Triggered after ph_shoes_etl DAG"}' \
        | jq

