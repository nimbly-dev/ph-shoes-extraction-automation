# .github/workflows/run-ph_shoes_etl.yml
name: Run ph_shoes_etl & dbt

on:
  workflow_dispatch:
    inputs:
      dag_id:
        description: 'Airflow DAG ID'
        required: true
        default: 'ph_shoes_etl'
      run_dbt:
        description: 'Trigger dbt Cloud downstream?'
        required: true
        type: choice
        options: ['true','false']
        default: 'true'

env:
  AWS_REGION:             ${{ vars.TF_VAR_AWS_REGION }}
  EC2_INSTANCE_NAME:      airflow-ec2
  AIRFLOW_API_SECRET_ARN: ${{ vars.AIRFLOW_API_SECRET_ARN }}

  AWS_ACCESS_KEY_ID:      ${{ secrets.AWS_ACCESS_KEY_ID }}
  AWS_SECRET_ACCESS_KEY:  ${{ secrets.AWS_SECRET_ACCESS_KEY }}
  DBT_ACCOUNT_ID:         ${{ secrets.DBT_CLOUD_ACCOUNT_ID }}
  DBT_JOB_ID:             ${{ secrets.DBT_CLOUD_JOB_ID }}
  DBT_API_TOKEN:          ${{ secrets.DBT_API_TOKEN }}

jobs:
  etl:
    runs-on: ubuntu-latest
    environment: main

    steps:
    - uses: actions/checkout@v3

    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v3
      with:
        aws-access-key-id:     ${{ env.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ env.AWS_SECRET_ACCESS_KEY }}
        aws-region:            ${{ env.AWS_REGION }}

    - name: Find Airflow EC2 instance
      id: ec2
      run: |
        INSTANCE_ID=$(aws ec2 describe-instances \
          --filters Name=tag:Name,Values="${EC2_INSTANCE_NAME}" Name=instance-state-name,Values=running \
          --query 'Reservations[0].Instances[0].InstanceId' --output text)
        if [[ -z "$INSTANCE_ID" || "$INSTANCE_ID" == "None" ]]; then
          echo "❌ No EC2 named ${EC2_INSTANCE_NAME} running" >&2
          exit 1
        fi
        echo "instance_id=$INSTANCE_ID" >> $GITHUB_OUTPUT

    - name: Trigger Airflow DAG (unpause & run)
      id: trigger
      run: |
        INSTANCE_ID=${{ steps.ec2.outputs.instance_id }}
        DAG_ID=${{ github.event.inputs.dag_id }}

        CMD_ID=$(aws ssm send-command \
          --region "$AWS_REGION" \
          --document-name "AWS-RunShellScript" \
          --instance-ids "$INSTANCE_ID" \
          --parameters '{"commands":[
            "set -euxo pipefail",
            "command -v jq >/dev/null 2>&1 || yum -y -q install jq",
            "CREDS=$(aws secretsmanager get-secret-value --secret-id \"'"${AIRFLOW_API_SECRET_ARN}"'\" --query SecretString --output text)",
            "USER=$(echo \"$CREDS\" | jq -r .username)",
            "PASS=$(echo \"$CREDS\" | jq -r .password)",
            "AUTH=$(printf \"%s:%s\" \"$USER\" \"$PASS\" | base64)",
            "RUN_ID=gh-$(date +%s)",
            "echo RUN_ID=$RUN_ID",
            "curl -fsS -X PATCH http://localhost:8080/api/v1/dags/$DAG_ID -H \"Authorization: Basic $AUTH\" -H \"Content-Type: application/json\" -d '{\"is_paused\":false}'",
            "curl -fsS -X POST http://localhost:8080/api/v1/dags/$DAG_ID/dagRuns -H \"Authorization: Basic $AUTH\" -H \"Content-Type: application/json\" -d '{\"dag_run_id\":\"'$RUN_ID'\",\"conf\":{}}'"
          ]}' \
          --query "Command.CommandId" --output text)

        aws ssm wait command-executed \
          --region "$AWS_REGION" \
          --instance-id "$INSTANCE_ID" \
          --command-id "$CMD_ID"

        OUT=$(aws ssm get-command-invocation \
          --region "$AWS_REGION" \
          --instance-id "$INSTANCE_ID" \
          --command-id "$CMD_ID" \
          --query StandardOutputContent --output text)
        echo "$OUT"
        RUN_ID=$(echo "$OUT" | awk -F= '/^RUN_ID=/{print $2}')
        echo "run_id=$RUN_ID" >> $GITHUB_OUTPUT

    - name: Poll Airflow DAG status
      run: |
        INSTANCE_ID=${{ steps.ec2.outputs.instance_id }}
        DAG_ID=${{ github.event.inputs.dag_id }}
        RUN_ID=${{ steps.trigger.outputs.run_id }}

        for i in $(seq 1 120); do
          STATUS=$(aws ssm send-command \
            --region "$AWS_REGION" \
            --document-name AWS-RunShellScript \
            --instance-ids "$INSTANCE_ID" \
            --parameters '{"commands":["curl -fsS http://localhost:8080/api/v1/dags/'"$DAG_ID"'/dagRuns/'"$RUN_ID"' -H \"Authorization: Basic $AUTH\" | jq -r .state"]}' \
            --query "Command.CommandId" --output text \
            | xargs -I{} aws ssm get-command-invocation \
                --region "$AWS_REGION" \
                --instance-id "$INSTANCE_ID" \
                --command-id {} \
                --query StandardOutputContent --output text)

          echo "Attempt #$i → $STATUS"
          if [[ "$STATUS" == success ]]; then
            echo "✅ DAG succeeded"; exit 0
          fi
          if [[ "$STATUS" == failed ]]; then
            echo "❌ DAG failed"; exit 1
          fi
          sleep 15
        done

        echo "⏰ Timed out waiting for DAG" >&2
        exit 1

    - name: Trigger dbt Cloud job
      if: ${{ success() && github.event.inputs.run_dbt == 'true' }}
      id: dbt
      run: |
        RESPONSE=$(curl -fsS -X POST \
          "https://cloud.getdbt.com/api/v2/accounts/${{ env.DBT_ACCOUNT_ID }}/jobs/${{ env.DBT_JOB_ID }}/run/" \
          -H "Authorization: Token ${{ env.DBT_API_TOKEN }}" \
          -H "Content-Type: application/json" \
          -d '{"cause":"After Airflow DAG '"${{ github.event.inputs.dag_id }}"'"}')
        DBT_RUN_ID=$(echo "$RESPONSE" | jq -r .data.id)
        echo "dbt_run_id=$DBT_RUN_ID" >> $GITHUB_OUTPUT

    - name: Wait for dbt Cloud run
      if: ${{ steps.dbt.outputs.dbt_run_id }}
      run: |
        for i in {1..60}; do
          STATUS=$(curl -fsS \
            "https://cloud.getdbt.com/api/v2/accounts/${{ env.DBT_ACCOUNT_ID }}/runs/${{ steps.dbt.outputs.dbt_run_id }}" \
            -H "Authorization: Token ${{ env.DBT_API_TOKEN }}" \
            | jq -r .data.attributes.status)
          echo "dbt attempt $i → $STATUS"
          [[ "$STATUS" == Success ]] && exit 0
          [[ "$STATUS" == Error   ]] && exit 1
          sleep 10
        done
        echo "❌ dbt run timed out" >&2
        exit 1
