# .github/workflows/run-ph-shoes-etl.yml
name: Run ph_shoes_etl

on:
  workflow_dispatch:
    inputs:
      dag_id:
        description: 'Airflow DAG ID'
        required: true
        default: 'ph_shoes_etl'

env:
  AWS_REGION:        ${{ vars.TF_VAR_AWS_REGION }}
  EC2_INSTANCE_NAME: ${{ vars.TF_VAR_EC2_INSTANCE_NAME }}

jobs:
  run-dag:
    runs-on: ubuntu-latest
    environment: main

    steps:
      - uses: actions/checkout@v3

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v3
        with:
          aws-access-key-id:     ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region:            ${{ env.AWS_REGION }}

      - name: Find Airflow EC2 instance
        id: ec2
        run: |
          INSTANCE_ID=$(aws ec2 describe-instances \
            --filters Name=tag:Name,Values="${EC2_INSTANCE_NAME}" \
                      Name=instance-state-name,Values=running \
            --query 'Reservations[0].Instances[0].InstanceId' --output text)
          if [[ -z "$INSTANCE_ID" || "$INSTANCE_ID" == "None" ]]; then
            echo "❌ No running EC2 named ${EC2_INSTANCE_NAME}" >&2
            exit 1
          fi
          echo "instance_id=$INSTANCE_ID" >> $GITHUB_OUTPUT

      - name: Install session-manager-plugin
        run: |
          sudo apt-get update -y
          sudo apt-get install -y session-manager-plugin

      - name: Trigger & unpause DAG via SSM
        id: trigger
        run: |
          INSTANCE_ID=${{ steps.ec2.outputs.instance_id }}
          DAG_ID=${{ github.event.inputs.dag_id }}
          RUN_ID="gh-$(date +%s)"

          # Array of commands, properly quoted
          COMMANDS=$(printf '["docker exec airflow-scheduler airflow dags unpause %s","docker exec airflow-scheduler airflow dags trigger %s --run-id %s"]' \
            "$DAG_ID" "$DAG_ID" "$RUN_ID")

          CMD_ID=$(aws ssm send-command \
            --region "$AWS_REGION" \
            --document-name AWS-RunShellScript \
            --instance-ids "$INSTANCE_ID" \
            --parameters commands="$COMMANDS" \
            --query "Command.CommandId" --output text)

          aws ssm wait command-executed \
            --region "$AWS_REGION" \
            --instance-id "$INSTANCE_ID" \
            --command-id "$CMD_ID"

          echo "run_id=$RUN_ID" >> $GITHUB_OUTPUT


      - name: Wait for DAG run to finish via SSM
        run: |
          INSTANCE_ID=${{ steps.ec2.outputs.instance_id }}
          DAG_ID=${{ github.event.inputs.dag_id }}
          RUN_ID=${{ steps.trigger.outputs.run_id }}

          for i in $(seq 1 10); do
            POLL_CMD="docker exec airflow-scheduler airflow dags list-runs -d ${DAG_ID} --output json"
            # Only one command, so pass as array
            POLL_ARRAY='["'"$POLL_CMD"'"]'

            CMD_ID=$(aws ssm send-command \
              --region "$AWS_REGION" \
              --document-name AWS-RunShellScript \
              --instance-ids "$INSTANCE_ID" \
              --parameters commands="$POLL_ARRAY" \
              --query "Command.CommandId" --output text)

            OUTPUT=$(aws ssm get-command-invocation \
              --region "$AWS_REGION" \
              --instance-id "$INSTANCE_ID" \
              --command-id "$CMD_ID" \
              --query StandardOutputContent --output text)

            echo "Raw list-runs output:"
            echo "$OUTPUT"

            # Now parse for your run id (do this locally in the workflow)
            STATE=$(echo "$OUTPUT" | jq -r ".[] | select(.run_id==\"${RUN_ID}\") | .state")

            echo "Attempt #$i → $STATE"
            [[ "$STATE" == success ]] && exit 0
            [[ "$STATE" == failed  ]] && exit 1
            sleep 15
          done

          echo "❌ Timed out waiting for DAG" >&2
          exit 1