name: Run ph_shoes_etl DAG & trigger dbt Cloud

on:
  workflow_dispatch:
    inputs:
      dag_id:
        description: 'Airflow DAG ID'
        required: true
        default: 'ph_shoes_etl'

env:
  AWS_REGION:        ap-southeast-1
  EC2_INSTANCE_NAME: airflow-ec2

jobs:
  run-and-trigger:
    runs-on: ubuntu-latest
    environment: main

    steps:
    - name: Checkout
      uses: actions/checkout@v3

    - name: Configure AWS creds for SSM
      uses: aws-actions/configure-aws-credentials@v3
      with:
        aws-access-key-id:     ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region:            ${{ env.AWS_REGION }}

    - name: Resolve EC2 instance ID
      id: get_id
      run: |
        ID=$(aws ec2 describe-instances \
          --filters "Name=tag:Name,Values=${EC2_INSTANCE_NAME}" \
                    "Name=instance-state-name,Values=running" \
          --query "Reservations[0].Instances[0].InstanceId" \
          --output text)
        echo "id=$ID" >> $GITHUB_OUTPUT

    - name: Trigger Airflow DAG via SSM
      id: send
      run: |
        INSTANCE_ID=${{ steps.get_id.outputs.id }}
        DAG_ID=${{ github.event.inputs.dag_id }}
        RUN_ID="gh-action-$(date +%s)"

        echo "▶ Unpause & trigger DAG $DAG_ID on $INSTANCE_ID (run-id=$RUN_ID)"
        CMD_ID=$(aws ssm send-command \
          --instance-ids "$INSTANCE_ID" \
          --document-name AWS-RunShellScript \
          --comment "Trigger $DAG_ID DAG" \
          --parameters commands='[
            "docker exec airflow-scheduler airflow dags unpause '"$DAG_ID"'",
            "docker exec airflow-scheduler airflow dags trigger '"$DAG_ID"' --run-id '"$RUN_ID"'"
          ]' \
          --region $AWS_REGION \
          --query "Command.CommandId" --output text)

        echo "cmd_id=$CMD_ID" >> $GITHUB_OUTPUT

    - name: Wait for Airflow trigger to finish
      run: |
        INSTANCE_ID=${{ steps.get_id.outputs.id }}
        CMD_ID=${{ steps.send.outputs.cmd_id }}

        for i in $(seq 1 60); do
          STATUS=$(aws ssm get-command-invocation \
            --command-id "$CMD_ID" \
            --instance-id "$INSTANCE_ID" \
            --region $AWS_REGION \
            --query "Status" --output text)
          if [[ "$STATUS" == "InProgress" ]] || [[ "$STATUS" == "Pending" ]]; then
            sleep 5; continue
          fi
          echo "▶ SSM trigger finished with status=$STATUS"
          if [[ "$STATUS" != "Success" ]]; then
            aws ssm get-command-invocation --command-id "$CMD_ID" --instance-id "$INSTANCE_ID" --region $AWS_REGION --output json
            exit 1
          fi
          break
        done

      - name: Wait for DAG run to complete
        run: |
          INSTANCE_ID=${{ steps.get_id.outputs.id }}
          DAG_ID=${{ github.event.inputs.dag_id }}
          RUN_ID="gh-action-${{ steps.send.outputs.cmd_id }}"

          echo "▶ Sleeping 10s for scheduler…"
          sleep 10

          echo "⏳ Polling for DAG run $RUN_ID…"
          for attempt in $(seq 1 30); do
            # 1) fire off list-runs
            CMD_ID=$(aws ssm send-command \
              --instance-ids "$INSTANCE_ID" \
              --document-name AWS-RunShellScript \
              --comment "Check DAG run $RUN_ID" \
              --parameters commands='[
                "docker exec airflow-scheduler bash -lc \"airflow dags list-runs -d '"$DAG_ID"' --no-color --output json || echo []\""
              ]' \
              --region $AWS_REGION \
              --query "Command.CommandId" --output text)

            echo "  → list-runs cmd_id=$CMD_ID"

            # 2) wait for SSM execution
            for poll in $(seq 1 24); do
              SSM_STAT=$(aws ssm get-command-invocation \
                --command-id "$CMD_ID" \
                --instance-id "$INSTANCE_ID" \
                --region $AWS_REGION \
                --query "Status" --output text)
              [ "$SSM_STAT" = "InProgress" ] && sleep 5 && continue
              break
            done

            # 3) grab the raw output
            RAW=$(aws ssm get-command-invocation \
              --command-id "$CMD_ID" \
              --instance-id "$INSTANCE_ID" \
              --region $AWS_REGION \
              --query "StandardOutputContent" --output text || echo "")

            echo "  ▶ RAW OUTPUT:"
            echo "$RAW" | sed 's/^/    /'

            # 4) strip ANSI and non-json, then parse
            CLEAN=$(echo "$RAW" \
              | sed -E 's/\x1B\[[0-9;]*[JKmsu]//g' \   # remove color codes
              | awk '/^\[/{ print; exit }'             # grab first line that looks like JSON array
            )
            JSON=${CLEAN:-"[]"}

            STATE=$(echo "$JSON" \
              | jq -r '.[] | select(.run_id=="'"$RUN_ID"'") | .state // ""' \
              || echo "")

            echo "  attempt #$attempt → parsed state='${STATE:-queued}'"
            if [ "$STATE" = "success" ]; then
              echo "✅ DAG run succeeded"
              exit 0
            elif [ "$STATE" = "failed" ]; then
              echo "❌ DAG run failed"
              exit 1
            fi

            sleep 15
          done

          echo "⏰ Timed out waiting for DAG run"
          exit 1


    - name: Trigger dbt Cloud job
      env:
        ACCOUNT_ID: ${{ secrets.DBT_CLOUD_ACCOUNT_ID }}
        JOB_ID:     ${{ secrets.DBT_CLOUD_JOB_ID }}
        API_TOKEN:  ${{ secrets.DBT_API_TOKEN }}
      run: |
        echo "▶ Triggering dbt Cloud job $JOB_ID on account $ACCOUNT_ID…"
        curl -sSf -X POST "https://wz482.us1.dbt.com/api/v2/accounts/${ACCOUNT_ID}/jobs/${JOB_ID}/run/" \
          -H "Authorization: Token ${API_TOKEN}" \
          -H "Content-Type: application/json" \
          -d '{"cause":"Triggered after ph_shoes_etl DAG"}' \
        | jq

