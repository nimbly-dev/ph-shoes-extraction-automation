name: Run ph_shoes_etl DAG & trigger dbt Cloud

on:
  workflow_dispatch:
    inputs:
      dag_id:
        description: 'Airflow DAG ID'
        required: true
        default: 'ph_shoes_etl'
      run_dbt:
        description: 'Also trigger the downstream dbt Cloud job?'
        required: true
        default: 'true'
        type: choice
        # GitHub only supports strings for choice, so use 'true'/'false'
        options:
          - 'true'
          - 'false'

env:
  AWS_REGION:        ap-southeast-1
  EC2_INSTANCE_NAME: airflow-ec2

jobs:
  run-and-trigger:
    runs-on: ubuntu-latest
    environment: main

    steps:
    - name: Checkout
      uses: actions/checkout@v3

    - name: Configure AWS creds for SSM
      uses: aws-actions/configure-aws-credentials@v3
      with:
        aws-access-key-id:     ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region:            ${{ env.AWS_REGION }}

    - name: Resolve EC2 instance ID
      id: get_id
      run: |
        ID=$(aws ec2 describe-instances \
          --filters "Name=tag:Name,Values=${EC2_INSTANCE_NAME}" \
                    "Name=instance-state-name,Values=running" \
          --query "Reservations[0].Instances[0].InstanceId" \
          --output text)
        echo "id=$ID" >> $GITHUB_OUTPUT

    - name: Trigger Airflow DAG via SSM
      id: send
      run: |
        INSTANCE_ID=${{ steps.get_id.outputs.id }}
        DAG_ID=${{ github.event.inputs.dag_id }}
        RUN_ID="gh-action-$(date +%s)"

        echo "▶ Unpause & trigger DAG $DAG_ID on $INSTANCE_ID (run-id=$RUN_ID)"
        CMD_ID=$(aws ssm send-command \
          --instance-ids "$INSTANCE_ID" \
          --document-name AWS-RunShellScript \
          --comment "Trigger $DAG_ID DAG" \
          --parameters commands='["docker exec airflow-scheduler airflow dags unpause '"$DAG_ID"'", "docker exec airflow-scheduler airflow dags trigger '"$DAG_ID"' --run-id '"$RUN_ID"'"]' \
          --region $AWS_REGION \
          --query "Command.CommandId" --output text)

        echo "cmd_id=$CMD_ID" >> $GITHUB_OUTPUT

    - name: Wait for Airflow trigger to finish
      run: |
        INSTANCE_ID=${{ steps.get_id.outputs.id }}
        CMD_ID=${{ steps.send.outputs.cmd_id }}

        for i in $(seq 1 60); do
          STATUS=$(aws ssm get-command-invocation \
            --command-id "$CMD_ID" \
            --instance-id "$INSTANCE_ID" \
            --region $AWS_REGION \
            --query "Status" --output text)
          if [[ "$STATUS" == "InProgress" ]] || [[ "$STATUS" == "Pending" ]]; then
            sleep 5
            continue
          fi
          echo "▶ SSM trigger finished with status=$STATUS"
          if [[ "$STATUS" != "Success" ]]; then
            aws ssm get-command-invocation \
              --command-id "$CMD_ID" \
              --instance-id "$INSTANCE_ID" \
              --region $AWS_REGION \
              --output json
            exit 1
          fi
          break
        done

    - name: Wait for DAG run to complete
      run: |
        INSTANCE_ID=${{ steps.get_id.outputs.id }}
        DAG_ID=${{ github.event.inputs.dag_id }}
        RUN_ID="gh-action-${{ steps.send.outputs.cmd_id }}"

        echo "▶ Sleeping 15 s so the scheduler registers the run…"
        sleep 15

        echo "⏳ Polling up to 30× for $DAG_ID run-id $RUN_ID → success / failed…"
        for attempt in $(seq 1 30); do
          # 1 – ask Airflow for that run’s state
          CMD_ID=$(aws ssm send-command \
            --instance-ids "$INSTANCE_ID" \
            --document-name AWS-RunShellScript \
            --comment "state $RUN_ID" \
            --parameters commands='[
              "docker exec airflow-scheduler bash -lc \"airflow dags state '"$DAG_ID"' --run-id '"$RUN_ID"' -q || echo queued\""
            ]' \
            --region $AWS_REGION \
            --query "Command.CommandId" --output text)

          # 2 – wait for the SSM invocation itself to finish
          for p in $(seq 1 18); do
            SSM_STAT=$(aws ssm get-command-invocation \
              --command-id "$CMD_ID" \
              --instance-id "$INSTANCE_ID" \
              --region $AWS_REGION \
              --query "Status" --output text)
            [[ "$SSM_STAT" =~ ^(Pending|InProgress)$ ]] && sleep 5 && continue
            break
          done

          # 3 – fetch the plain-text state (one word)
          STATE=$(aws ssm get-command-invocation \
            --command-id "$CMD_ID" \
            --instance-id "$INSTANCE_ID" \
            --region $AWS_REGION \
            --query "StandardOutputContent" --output text | tr -d '\r\n ')

          echo "  attempt #$attempt → state='${STATE}'"

          case "$STATE" in
            success) echo "✅ DAG run succeeded"; exit 0 ;;
            failed)  echo "❌ DAG run failed"   ; exit 1 ;;
            running|queued|"") sleep 15 ;;   # keep waiting
            *) echo "⚠︎ got unexpected state '$STATE'; still waiting"; sleep 15 ;;
          esac
        done

        echo "⏰ Timed out waiting for DAG run"
        exit 1


    - name: Trigger dbt Cloud job
      if: ${{ github.event.inputs.run_dbt == 'true' }}
      env:
        ACCOUNT_ID: ${{ secrets.DBT_CLOUD_ACCOUNT_ID }}
        JOB_ID:     ${{ secrets.DBT_CLOUD_JOB_ID }}
        API_TOKEN:  ${{ secrets.DBT_API_TOKEN }}
      run: |
        echo "▶ Triggering dbt Cloud job $JOB_ID on account $ACCOUNT_ID…"
        curl -sSf -X POST "https://wz482.us1.dbt.com/api/v2/accounts/${ACCOUNT_ID}/jobs/${JOB_ID}/run/" \
          -H "Authorization: Token ${API_TOKEN}" \
          -H "Content-Type: application/json" \
          -d '{"cause":"Triggered after ph_shoes_etl DAG"}' \
        | jq

