# .github/workflows/deploy-airflow.yml
name: Airflow Full CD

on:
  workflow_dispatch:
    inputs:
      aws_region:
        description: 'AWS Region'
        required: true
        default: 'ap-southeast-1'
      ec2_key_name:
        description: 'EC2 KeyPair name'
        required: true
        default: 'ec2-ph-shoes-automation-keypair-name'
      ec2_instance_name:
        description: 'EC2 instance Name tag'
        required: true
        default: 'airflow-ec2'
      artifact_bucket_name:
        description: 'S3 bucket for CodeDeploy artifacts'
        required: true
        default: 'ph-shoes-airflow-artifacts'

env:
  S3_BUCKET: ${{ github.event.inputs.artifact_bucket_name }}
  S3_KEY:    deployment/deployment.zip

jobs:

  build-artifacts:
    name: Build & Package
    runs-on: ubuntu-latest

    # ← here we bind to your “main” environment
    environment:
      name: main

    steps:
      - uses: actions/checkout@v3

      - name: Configure AWS credentials for S3
        uses: aws-actions/configure-aws-credentials@v3
        with:
          aws-access-key-id:     ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region:            ${{ github.event.inputs.aws_region }}

      - name: Build scheduler image & save to tar
        run: |
          docker build -t ph_shoes_airflow_scheduler:latest airflow_dags
          docker save ph_shoes_airflow_scheduler:latest -o ph_shoes_airflow_scheduler.tar

      - name: Archive DAGs folder
        run: tar czf dags.tar.gz -C airflow_dags .

      - name: Prepare deployment.zip
        run: |
          mkdir deploy_pkg
          cp deployment/appspec.yml         deploy_pkg/
          cp deployment/scripts/*.sh        deploy_pkg/
          cp dags.tar.gz                    deploy_pkg/
          cp ph_shoes_airflow_scheduler.tar deploy_pkg/
          cd deploy_pkg && zip -r ../deployment.zip .

      - name: Upload deployment.zip to S3
        run: aws s3 cp deployment.zip s3://$S3_BUCKET/$S3_KEY


  deploy-ec2-airflow:
    name: Provision / Replace EC2 (Airflow)
    needs: build-artifacts
    runs-on: ubuntu-latest

    environment:
      name: main

    env:
      TF_VAR_aws_region:        ${{ github.event.inputs.aws_region }}
      TF_VAR_ec2_key_name:      ${{ github.event.inputs.ec2_key_name }}
      TF_VAR_ec2_instance_name: ${{ github.event.inputs.ec2_instance_name }}
      TF_VAR_environment:       prod

    steps:
      - uses: actions/checkout@v3

      - name: Configure AWS credentials for Terraform
        uses: aws-actions/configure-aws-credentials@v3
        with:
          aws-access-key-id:     ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region:            ${{ github.event.inputs.aws_region }}

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v2
        with:
          terraform_version: 1.5.6

      - name: Terraform Init
        working-directory: terraform-ec2-airflow
        run: terraform init -input=false -reconfigure

      - name: Conditionally import existing EC2 KeyPair
        working-directory: terraform-ec2-airflow
        run: |
          # first list what AWS thinks exists
          echo "→ KeyPairs in ${TF_VAR_aws_region}:"
          aws ec2 describe-key-pairs \
            --region ${TF_VAR_aws_region} \
            --query 'KeyPairs[*].KeyName' --output table

          # only import if it really exists
          if aws ec2 describe-key-pairs \
               --region ${TF_VAR_aws_region} \
               --key-names "${TF_VAR_ec2_key_name}" \
               --output text >/dev/null 2>&1; then
            echo "Importing key-pair ${TF_VAR_ec2_key_name}…"
            terraform state rm module.ec2_instance.aws_key_pair.automation_key[0] || true
            terraform import module.ec2_instance.aws_key_pair.automation_key[0] "${TF_VAR_ec2_key_name}"
          else
            echo "KeyPair ${TF_VAR_ec2_key_name} not found in AWS – Terraform will create it."
          fi


      - name: Conditionally import existing SG
        working-directory: terraform-ec2-airflow
        run: |
          SG_NAME="${TF_VAR_ec2_instance_name}-sg"
          echo "→ Security-groups named $SG_NAME:"
          aws ec2 describe-security-groups \
            --region ${TF_VAR_aws_region} \
            --filters Name=group-name,Values="$SG_NAME" \
            --query 'SecurityGroups[*].GroupId' --output table

          SG_ID=$(aws ec2 describe-security-groups \
                    --region ${TF_VAR_aws_region} \
                    --filters Name=group-name,Values="$SG_NAME" \
                    --query "SecurityGroups[0].GroupId" --output text)

          if [ -n "$SG_ID" ] && [ "$SG_ID" != "None" ]; then
            echo "Importing SG $SG_NAME → $SG_ID"
            terraform state rm module.ec2_instance.aws_security_group.this[0] || true
            terraform import module.ec2_instance.aws_security_group.this[0] $SG_ID
          else
            echo "SG $SG_NAME not found – Terraform will create it."
          fi



      - name: Terraform Apply
        working-directory: terraform-ec2-airflow
        run: terraform apply -auto-approve


  trigger-codedeploy:
    name: Trigger CodeDeploy
    needs: deploy-ec2-airflow
    runs-on: ubuntu-latest

    environment:
      name: main

    steps:
      - uses: actions/checkout@v3

      - name: Configure AWS credentials for CodeDeploy
        uses: aws-actions/configure-aws-credentials@v3
        with:
          aws-access-key-id:     ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region:            ${{ github.event.inputs.aws_region }}

      - name: Create CodeDeploy deployment
        run: |
          aws deploy create-deployment \
            --application-name ph-shoes-airflow-codedeploy-app \
            --deployment-group-name ph-shoes-airflow-deployment-group \
            --s3-location bucket=$S3_BUCKET,bundleType=zip,key=$S3_KEY \
            --region ${{ github.event.inputs.aws_region }}
