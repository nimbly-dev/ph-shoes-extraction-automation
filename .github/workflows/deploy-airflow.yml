# .github/workflows/deploy-airflow.yml
name: Airflow Full CD

on:
  workflow_dispatch:
    inputs:
      aws_region:
        description: 'AWS Region'
        required: true
        default: 'ap-southeast-1'
      ec2_key_name:
        description: 'EC2 KeyPair name'
        required: true
        default: 'ec2-ph-shoes-automation-keypair-name'
      ec2_instance_name:
        description: 'EC2 instance Name tag'
        required: true
        default: 'airflow-ec2'
      artifact_bucket_name:
        description: 'S3 bucket for CodeDeploy artifacts'
        required: true
        default: 'ph-shoes-airflow-artifacts'

env:
  S3_BUCKET: ${{ github.event.inputs.artifact_bucket_name }}
  S3_KEY:    deployment/deployment.zip

jobs:

  build-artifacts:
    name: Build & Package
    runs-on: ubuntu-latest
    environment: 
      name: main

    steps:
      - uses: actions/checkout@v3

      - name: Configure AWS credentials for S3
        uses: aws-actions/configure-aws-credentials@v3
        with:
          aws-access-key-id:     ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region:            ${{ github.event.inputs.aws_region }}

      - name: Build scheduler image & save to tar
        run: |
          docker build -t ph_shoes_airflow_scheduler:latest airflow_dags
          docker save ph_shoes_airflow_scheduler:latest -o ph_shoes_airflow_scheduler.tar

      - name: Archive DAGs folder
        run: tar czf dags.tar.gz -C airflow_dags .

      - name: Prepare deployment.zip
        run: |
          rm -rf deploy_pkg
          mkdir -p deploy_pkg/scripts
          cp deployment/appspec.yml         deploy_pkg/
          cp deployment/scripts/*.sh        deploy_pkg/scripts/
          cp dags.tar.gz                    deploy_pkg/
          cp ph_shoes_airflow_scheduler.tar deploy_pkg/
          cd deploy_pkg && zip -r ../deployment.zip .

      - name: Upload deployment.zip to S3 (remove then upload)
            run: |
              set -euxo pipefail
              aws s3 rm s3://$S3_BUCKET/$S3_KEY --region ${{ github.event.inputs.aws_region }} || true
              aws s3 cp deployment.zip s3://$S3_BUCKET/$S3_KEY --region ${{ github.event.inputs.aws_region }}


  deploy-ec2-airflow:
    name: Provision / Replace EC2 (Airflow)
    needs: build-artifacts
    runs-on: ubuntu-latest
    environment:
      name: main

    env:
      TF_VAR_aws_region:        ${{ github.event.inputs.aws_region }}
      TF_VAR_ec2_key_name:      ${{ github.event.inputs.ec2_key_name }}
      TF_VAR_ec2_instance_name: ${{ github.event.inputs.ec2_instance_name }}
      TF_VAR_environment:       prod

    steps:
      - uses: actions/checkout@v3

      - name: Configure AWS credentials for Terraform
        uses: aws-actions/configure-aws-credentials@v3
        with:
          aws-access-key-id:     ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region:            ${{ github.event.inputs.aws_region }}

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v2
        with:
          terraform_version: 1.5.6

      - name: Terraform Init
        working-directory: terraform-ec2-airflow
        run: terraform init -input=false -reconfigure

      - name: Conditionally import existing EC2 KeyPair
        working-directory: terraform-ec2-airflow
        run: |
          echo "â†’ KeyPairs in ${TF_VAR_aws_region}:"
          aws ec2 describe-key-pairs --region ${TF_VAR_aws_region} \
            --query 'KeyPairs[*].KeyName' --output table

          if aws ec2 describe-key-pairs --region ${TF_VAR_aws_region} \
               --key-names "${TF_VAR_ec2_key_name}" >/dev/null 2>&1; then
            terraform state rm module.ec2_instance.aws_key_pair.automation_key[0] || true
            terraform import module.ec2_instance.aws_key_pair.automation_key[0] "${TF_VAR_ec2_key_name}"
          fi

      - name: Conditionally import existing SG
        working-directory: terraform-ec2-airflow
        run: |
          SG_NAME="${TF_VAR_ec2_instance_name}-sg"
          echo "â†’ Security-groups named $SG_NAME:"
          aws ec2 describe-security-groups --region ${TF_VAR_aws_region} \
            --filters Name=group-name,Values="$SG_NAME" \
            --query 'SecurityGroups[*].GroupId' --output table

          SG_ID=$(aws ec2 describe-security-groups --region ${TF_VAR_aws_region} \
                    --filters Name=group-name,Values="$SG_NAME" \
                    --query "SecurityGroups[0].GroupId" --output text)

          if [ -n "$SG_ID" ] && [ "$SG_ID" != "None" ]; then
            terraform state rm module.ec2_instance.aws_security_group.this[0] || true
            terraform import module.ec2_instance.aws_security_group.this[0] $SG_ID
          fi

      - name: Terraform Apply
        working-directory: terraform-ec2-airflow
        run: terraform apply -auto-approve


  trigger-codedeploy:
    name: Trigger & Wait CodeDeploy
    needs: deploy-ec2-airflow
    runs-on: ubuntu-latest
    environment:
      name: main

    steps:
      - uses: actions/checkout@v3

      - name: Configure AWS credentials for CodeDeploy
        uses: aws-actions/configure-aws-credentials@v3
        with:
          aws-access-key-id:     ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region:            ${{ github.event.inputs.aws_region }}

      - name: Trigger CodeDeploy (PowerShell)
        shell: pwsh
        env:
          AWS_REGION: ${{ github.event.inputs.aws_region }}
          BUCKET:     ${{ github.event.inputs.artifact_bucket_name }}
          KEY:        deployment/deployment.zip
        run: |
          Set-StrictMode -Version Latest
          $ErrorActionPreference = 'Stop'

          # 1) Remove & re-upload (we already did in build job, skip here if you like)
          aws s3 rm "s3://$env:BUCKET/$env:KEY" --region $env:AWS_REGION
          aws s3 cp "$PWD/deployment.zip" "s3://$env:BUCKET/$env:KEY" --region $env:AWS_REGION

          # 2) Trigger CodeDeploy
          $app   = 'ph-shoes-airflow-codedeploy-app'
          $group = 'ph-shoes-airflow-deployment-group'
          $s3loc = "bucket=$env:BUCKET,bundleType=zip,key=$env:KEY"

          Write-Host "ðŸŽ¯ Creating CodeDeploy deployment..."
          $resp = aws deploy create-deployment `
            --application-name      $app `
            --deployment-group-name $group `
            --s3-location           $s3loc `
            --file-exists-behavior OVERWRITE `
            --region                $env:AWS_REGION | ConvertFrom-Json

          $deployId = $resp.deploymentId
          Write-Host "Deployment started: $deployId"

          # 3) Wait for it to finish
          aws deploy wait deployment-successful `
            --deployment-id $deployId `
            --region         $env:AWS_REGION

          Write-Host "âœ… CodeDeploy deployment $deployId succeeded."